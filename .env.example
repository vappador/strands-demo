# Model provider (choose one):
#   Bedrock (AWS creds), or
#   STRANDS_MODEL_PROVIDER=openai ; OPENAI_API_KEY=sk-...
#   STRANDS_MODEL_PROVIDER=ollama ; OLLAMA_HOST=http://host.docker.internal:11434 ; STRANDS_MODEL=llama3.1:8b
STRANDS_MODEL_PROVIDER=ollama
OLLAMA_HOST=http://localhost:11434
STRANDS_MODEL=qwen2.5-coder:3b

#__SET_AT_RUNTIME__
GITHUB_TOKEN=
OTEL_EXPORTER_OTLP_ENDPOINT=
OTEL_CONSOLE_EXPORT=1
WORKSPACE_DIR=/workspace/jobs

# Search-context limits
# maximum number of file snippets to return
SEARCH_CONTEXT_MAX_RESULTS=20
# maximum total characters across all snippets
SEARCH_CONTEXT_MAX_CHARS=8000
# lines of context to include around each match
SEARCH_CONTEXT_LINES=2
